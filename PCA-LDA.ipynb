{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from warnings import filterwarnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "from math import log\n",
    "\n",
    "# Disable warnings from being printed\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileN = 800\n",
    "fileM = 100000\n",
    "\n",
    "def read_data(filename):\n",
    "    data = pd.DataFrame(columns=range(fileM))\n",
    "    with open(filename, 'r') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "        for i in range(len(lines)):\n",
    "            record = np.fromstring(lines[i], dtype=int, sep=' ')\n",
    "            record_bool = [0 for i in range(fileM)]\n",
    "            for col in record:\n",
    "                record_bool[col-1] = 1\n",
    "            data.loc[i] = record_bool\n",
    "    return data\n",
    "\n",
    "def read_labels(filename):\n",
    "    labels = []\n",
    "    with open(filename, 'r') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "        for line in lines:\n",
    "            labels.append(np.fromstring(line, dtype=int, sep=' ')[0])\n",
    "    return labels\n",
    "\n",
    "# Read the data into dataframe\n",
    "train_data = read_data(\"dorothea/dorothea_train.data\")\n",
    "valid_data = read_data(\"dorothea/dorothea_valid.data\")\n",
    "\n",
    "# Get the labels of the train data\n",
    "train_data_labels = read_labels(\"dorothea/dorothea_train.labels\")\n",
    "valid_data_labels = read_labels(\"dorothea/dorothea_valid.labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute data which is constant in different runs of pca, i.e. eigenvectors\n",
    "\n",
    "def compute_eigenvectors(data):\n",
    "    \n",
    "    # Center the data around mean\n",
    "    data_centered = data - np.mean(data, axis=0)\n",
    "\n",
    "    # Compute the covariance matrix (xx' i.e nXn), and find eigenvalues and eigenvectors\n",
    "    cov_matrix = np.cov(data_centered)\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "    # Now eigenvectors of x'x matrix can be obtained from these by multiplying by x', eigenvalues remain same\n",
    "    eigenvectors = np.dot(np.transpose(data_centered), eigenvectors)\n",
    "\n",
    "    # Sort the eigenvectors in decreasing order of eigenvalues\n",
    "    sort_order = np.argsort(eigenvalues)[::-1]\n",
    "    new_eigenvectors = np.zeros(eigenvectors.shape)\n",
    "    for i in range(eigenvalues.shape[0]):\n",
    "        new_eigenvectors[:, i] = eigenvectors[:, sort_order[i]]\n",
    "        \n",
    "    return new_eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get data in the new feature space of reduced dimensionality.\n",
    "def pca_(data, new_eigenvectors, k):\n",
    "    \n",
    "    # Get first K eigenvectors\n",
    "    eigenvectors_firstK = new_eigenvectors[:, :k]\n",
    "    \n",
    "    # Get data in reduced dimension space\n",
    "    projected_data = np.dot(data, eigenvectors_firstK)\n",
    "    \n",
    "    return pd.DataFrame(projected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GNBC(train, valid):\n",
    "    \n",
    "    # Separate the classes\n",
    "    class_m = train[train[\"labels\"] == -1]\n",
    "    class_p = train[train[\"labels\"] == 1]\n",
    "    \n",
    "    # Calculate prior probabilities for both classes\n",
    "    prior_m = class_m.shape[0]/train.shape[0]\n",
    "    prior_p = class_p.shape[0]/train.shape[0]\n",
    "    \n",
    "    # Calculate variances for all features\n",
    "    var_m = np.var(class_m, axis=0)\n",
    "    var_p = np.var(class_p, axis=0)\n",
    "    \n",
    "    # Calculate mean for all features\n",
    "    mean_m = np.mean(class_m, axis=0)\n",
    "    mean_p = np.mean(class_p, axis=0)\n",
    "\n",
    "    # Predict\n",
    "    results = []\n",
    "\n",
    "    for i in range(valid.shape[0]):\n",
    "\n",
    "        posterior_m = log(prior_m)\n",
    "        posterior_p = log(prior_p)\n",
    "        \n",
    "        for j in range(valid.shape[1]-1):\n",
    "            cur_x = valid.loc[i, j]\n",
    "            posterior_m = posterior_m + (-0.5 * (((cur_x - mean_m[j])**2) / var_m[j])) - 0.5*log(var_m[j])\n",
    "            posterior_p = posterior_p + (-0.5 * (((cur_x - mean_p[j])**2) / var_p[j])) - 0.5*log(var_p[j])\n",
    "\n",
    "        if posterior_m >= posterior_p:\n",
    "            cur_class = -1\n",
    "        else:\n",
    "            cur_class = 1\n",
    "    \n",
    "        results.append(cur_class)\n",
    "        \n",
    "    # Calculate accuracy\n",
    "    return accuracy_score(valid[\"labels\"], results)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics\n",
      "100 0.834285714286\n",
      "500 0.84\n",
      "800 0.84\n"
     ]
    }
   ],
   "source": [
    "def iterate_pca(train_data, valid_data, train_data_labels, valid_data_labels):\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    kl = [100, 500, 800]\n",
    "    \n",
    "    new_eigenvectors_train = compute_eigenvectors(train_data)\n",
    "    new_eigenvectors_valid = compute_eigenvectors(valid_data)\n",
    "    \n",
    "    for k in kl:\n",
    "        projected_train = pca_(train_data, new_eigenvectors_train, k)\n",
    "        projected_valid = pca_(valid_data, new_eigenvectors_valid, k)\n",
    "    \n",
    "        projected_train[\"labels\"] = train_data_labels\n",
    "        projected_valid[\"labels\"] = valid_data_labels\n",
    "\n",
    "        cur_accuracy = GNBC_pca(projected_train, projected_valid)\n",
    "        accuracies.append(cur_accuracy)\n",
    "        \n",
    "    print(\"Statistics\")\n",
    "    print(100, accuracies[0])\n",
    "    print(500, accuracies[1])\n",
    "    print(800, accuracies[2])\n",
    "    \n",
    "iterate_pca(train_data, valid_data, train_data_labels, valid_data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lda_(data):\n",
    "    # Separate the train data classwise.\n",
    "    class_m = data[data[\"labels\"] == -1]\n",
    "    class_p = data[data[\"labels\"] == 1]\n",
    "\n",
    "    # Drop the last labels column for matrix calculations\n",
    "    class_m = class_m.drop(\"labels\", axis=1)\n",
    "    class_p = class_p.drop(\"labels\", axis=1)\n",
    "\n",
    "    # Get scatter matrices for each class separately\n",
    "    scatter_m = np.cov(np.transpose(class_m))\n",
    "    scatter_p = np.cov(np.transpose(class_p))\n",
    "\n",
    "    # Compute means for each feature.\n",
    "    mean_m = np.mean(class_m, axis=0)\n",
    "    mean_p = np.mean(class_p, axis=0)\n",
    "    mean_t = np.mean(data, axis=0)\n",
    "    mean_t = mean_t.drop(\"labels\")\n",
    "\n",
    "    # Compute with class and between class scatter matrices\n",
    "    sw = scatter_m + scatter_p\n",
    "    swin = np.linalg.inv(sw)\n",
    "    wstar = np.dot(swin, (mean_m - mean_p))\n",
    "\n",
    "    # Find new projected data\n",
    "    new_projected_data = data.drop(\"labels\", axis=1)\n",
    "    new_projected_data = np.dot(np.transpose(wstar), new_projected_data)\n",
    "    return pd.DataFrame(new_projected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.902857142857\n"
     ]
    }
   ],
   "source": [
    "def iterate_lda(train_data, valid_data, train_data_labels, valid_data_labels):\n",
    "    \n",
    "    # Get projected data as input for LDA\n",
    "    new_eigenvectors_train = compute_eigenvectors(train_data)\n",
    "    new_eigenvectors_valid = compute_eigenvectors(valid_data)\n",
    "    projected_train = pca_(train_data, new_eigenvectors_train, 800)\n",
    "    projected_valid = pca_(valid_data, new_eigenvectors_valid, 800)\n",
    "    projected_train[\"labels\"] = train_data_labels\n",
    "    projected_valid[\"labels\"] = valid_data_labels\n",
    "    \n",
    "    # Get LDA applied projected data\n",
    "    new_projected_train = lda_(projected_train)\n",
    "    new_projected_valid = lda_(projected_valid)\n",
    "    new_projected_train[\"labels\"] = train_data_labels\n",
    "    new_projected_valid[\"labels\"] = valid_data_labels\n",
    "    \n",
    "    accuracy = GNBC(new_projected_train, new_projected_valid)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    \n",
    "iterate_lda(train_data, valid_data, train_data_labels, valid_data_labels)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
